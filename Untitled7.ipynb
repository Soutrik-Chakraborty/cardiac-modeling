{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soutrik-Chakraborty/cardiac-modeling/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtEErX_Z5lO1",
        "outputId": "695afa98-b22b-4366-b0fb-2b38832fc760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (5.3.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Collecting vedo\n",
            "  Downloading vedo-2025.5.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from nibabel) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (1.16.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n",
            "Collecting vtk (from vedo)\n",
            "  Downloading vtk-9.5.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: Pygments in /usr/local/lib/python3.12/dist-packages (from vedo) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from vtk->vedo) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->vtk->vedo) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->vtk->vedo) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->vtk->vedo) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->vtk->vedo) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->vtk->vedo) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->vtk->vedo) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->vtk->vedo) (1.17.0)\n",
            "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vedo-2025.5.4-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vtk-9.5.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (112.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom, vtk, vedo\n",
            "Successfully installed pydicom-3.0.1 vedo-2025.5.4 vtk-9.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install pydicom nibabel opencv-python numpy torch torchvision scikit-image vedo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pydicom\n",
        "import nibabel as nib\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.ndimage import zoom\n",
        "from skimage import measure\n",
        "from vedo import Plotter, Mesh, Volume"
      ],
      "metadata": {
        "id": "N5VAG4I9P1Kv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip_rvsc_dataset(zip_path, extract_to):\n",
        "    if not os.path.isfile(zip_path):\n",
        "        raise FileNotFoundError(f\"Zip file not found at: {zip_path}\")\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(f\"\\u2705 Extracted dataset to: {extract_to}\")"
      ],
      "metadata": {
        "id": "SQg1ChGARJIQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dicom_series(dicom_folder):\n",
        "    slices = []\n",
        "    for fname in sorted(os.listdir(dicom_folder)):\n",
        "        if fname.endswith(\".dcm\"):\n",
        "            path = os.path.join(dicom_folder, fname)\n",
        "            dcm = pydicom.dcmread(path)\n",
        "            slices.append((dcm, int(fname.split('-')[1].split('.')[0])))\n",
        "    slices.sort(key=lambda x: x[1])\n",
        "    images = np.stack([s.pixel_array for s, _ in slices])\n",
        "    spacing = float(slices[0][0].PixelSpacing[0])\n",
        "    # Calculate slice thickness. Handle potential missing or inconsistent SliceLocation\n",
        "    thickness = 0.0\n",
        "    if len(slices) > 1 and hasattr(slices[0][0], 'SliceLocation') and hasattr(slices[1][0], 'SliceLocation'):\n",
        "        thickness = abs(slices[1][0].SliceLocation - slices[0][0].SliceLocation)\n",
        "    if thickness == 0.0 and hasattr(slices[0][0], 'SliceThickness'):\n",
        "         thickness = float(slices[0][0].SliceThickness)\n",
        "    if thickness == 0.0:\n",
        "        print(\"Warning: Could not determine slice thickness from DICOM headers. Using a default value of 1.0.\")\n",
        "        thickness = 1.0 # Default value if thickness cannot be determined\n",
        "\n",
        "\n",
        "    return images, spacing, thickness, [s[1] for s in slices]\n",
        "\n",
        "def load_contour_file(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        coords = [list(map(float, line.strip().split())) for line in f if line.strip()]\n",
        "    if not coords:\n",
        "        return np.array([], dtype=np.int32)\n",
        "    return np.array(coords, dtype=np.int32)\n",
        "\n",
        "def create_mask(image_shape, contour):\n",
        "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
        "    if contour.size > 0:\n",
        "        contour = contour.reshape((-1, 1, 2)).astype(np.int32)\n",
        "        cv2.fillPoly(mask, [contour], color=1)\n",
        "    return mask\n",
        "\n",
        "def contours_to_mask(contour_folder, dicom_numbers, image_shape, label='i'):\n",
        "    mask_volume = np.zeros((len(dicom_numbers), *image_shape), dtype=np.uint8)\n",
        "    for idx, num in enumerate(dicom_numbers):\n",
        "        fname = f\"{label}contour-manual.txt\"\n",
        "        files = glob.glob(os.path.join(contour_folder, f\"*{num:04d}*{fname}\"))\n",
        "        if files:\n",
        "            contour = load_contour_file(files[0])\n",
        "            mask = create_mask(image_shape, contour)\n",
        "            mask_volume[idx] = mask\n",
        "    return mask_volume\n",
        "\n",
        "def save_as_nifti(volume, output_path, spacing, slice_thickness):\n",
        "    affine = np.diag([spacing, spacing, slice_thickness, 1])\n",
        "    # Ensure determinant is not zero for affine decomposition\n",
        "    if np.linalg.det(affine) == 0:\n",
        "        print(f\"Warning: Affine determinant is zero. Using identity matrix with spacing for NIfTI saving.\")\n",
        "        affine = np.diag([spacing, spacing, slice_thickness, 1]) # Recreate affine just in case, though logic above should prevent det=0\n",
        "        # Fallback to a robust affine if the above still fails - this might not be ideal for all data but prevents crashing\n",
        "        if np.linalg.det(affine) == 0:\n",
        "             affine = np.eye(4)\n",
        "             affine[0,0] = spacing\n",
        "             affine[1,1] = spacing\n",
        "             affine[2,2] = slice_thickness\n",
        "\n",
        "    nib.save(nib.Nifti1Image(volume.astype(np.int16), affine), output_path)\n",
        "\n",
        "\n",
        "def convert_all_patients_to_nifti(input_root, output_root):\n",
        "    os.makedirs(output_root, exist_ok=True)\n",
        "    patient_dirs = sorted([d for d in os.listdir(input_root) if d.startswith(\"patient\")])\n",
        "    for patient in patient_dirs:\n",
        "        p_path = os.path.join(input_root, patient)\n",
        "        p_num = patient.replace(\"patient\", \"\")\n",
        "        dicom_folder = os.path.join(p_path, f\"P{p_num}dicom\")\n",
        "        contour_folder = os.path.join(p_path, f\"P{p_num}contours-manual\")\n",
        "        output_dir = os.path.join(output_root, f\"P{p_num}\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        try:\n",
        "            img, spc, thk, nums = load_dicom_series(dicom_folder)\n",
        "            msk = contours_to_mask(contour_folder, nums, img.shape[1:], label='i')\n",
        "            save_as_nifti(img, os.path.join(output_dir, \"image.nii.gz\"), spc, thk)\n",
        "            save_as_nifti(msk, os.path.join(output_dir, \"mask.nii.gz\"), spc, thk)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {patient}: {e}\")"
      ],
      "metadata": {
        "id": "xs2Byx-JP3f8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_nii_gz_to_nii(folder):\n",
        "    nii_gz_files = glob.glob(os.path.join(folder, \"**\", \"*.nii.gz\"), recursive=True)\n",
        "    for gz in nii_gz_files:\n",
        "        data = nib.load(gz)\n",
        "        nii_path = gz.replace(\".nii.gz\", \".nii\")\n",
        "        nib.save(data, nii_path)\n",
        "        os.remove(gz)\n",
        "        print(f\"Converted and removed: {gz}\")"
      ],
      "metadata": {
        "id": "f0HJJWnYQHQU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def organize_dataset_for_training(nifti_root, output_root):\n",
        "    os.makedirs(os.path.join(output_root, \"train\", \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_root, \"train\", \"masks\"), exist_ok=True)\n",
        "    patient_folders = sorted(glob.glob(os.path.join(nifti_root, \"P*\")))\n",
        "    for i, pf in enumerate(patient_folders):\n",
        "        img = glob.glob(os.path.join(pf, \"image.nii\"))[0]\n",
        "        msk = glob.glob(os.path.join(pf, \"mask.nii\"))[0]\n",
        "        pid = f\"P{str(i+1).zfill(2)}\"\n",
        "        shutil.copy(img, os.path.join(output_root, \"train\", \"images\", f\"{pid}_image.nii\"))\n",
        "        shutil.copy(msk, os.path.join(output_root, \"train\", \"masks\", f\"{pid}_mask.nii\"))\n",
        "    print(\"✅ Dataset organized for training.\")"
      ],
      "metadata": {
        "id": "qlERMfj6QO2f"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#New Code and run it\n",
        "class HVSMR3DDataset(Dataset):\n",
        "    def __init__(self, root, split=\"train\"):\n",
        "        images = sorted(glob.glob(os.path.join(root, split, \"images\", \"*.nii\")))\n",
        "        masks  = sorted(glob.glob(os.path.join(root, split, \"masks\", \"*.nii\")))\n",
        "        self.pairs = list(zip(images, masks))\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, msk_path = self.pairs[idx]\n",
        "        img = nib.load(img_path).get_fdata().astype(np.float32)\n",
        "        msk = nib.load(msk_path).get_fdata().astype(np.int64)\n",
        "\n",
        "        # Resize to a fixed size, e.g., 128x128x128\n",
        "        target_shape = (128, 128, 128)\n",
        "\n",
        "        # Ensure zoom factors are calculated correctly\n",
        "        zoom_factors = np.array(target_shape) / np.array(img.shape)\n",
        "\n",
        "        # Interpolation: order=1 for image (trilinear), order=0 for mask (nearest neighbor)\n",
        "        img = zoom(img, zoom_factors, order=1)\n",
        "        msk = zoom(msk, zoom_factors, order=0)\n",
        "\n",
        "        # Normalization\n",
        "        img = (img - img.mean()) / (img.std() + 1e-8) # Add epsilon to avoid division by zero\n",
        "\n",
        "        # Add channel dimension\n",
        "        return torch.from_numpy(img).unsqueeze(0), torch.from_numpy(msk)\n",
        "\n",
        "# Helper module for the U-Net\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "# Improved U-Net with skip connections and interpolation-based upsampling\n",
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, in_ch=1, out_ch=2):\n",
        "        super(UNet3D, self).__init__()\n",
        "\n",
        "        # Encoder Path\n",
        "        self.inc = DoubleConv(in_ch, 64)\n",
        "        self.down1 = nn.Sequential(nn.MaxPool3d(2), DoubleConv(64, 128))\n",
        "        self.down2 = nn.Sequential(nn.MaxPool3d(2), DoubleConv(128, 256))\n",
        "\n",
        "        # Decoder Path (uses interpolation)\n",
        "        self.up1 = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
        "        self.conv1 = DoubleConv(256 + 128, 128)\n",
        "\n",
        "        self.up2 = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
        "        self.conv2 = DoubleConv(128 + 64, 64)\n",
        "\n",
        "        # Output layer\n",
        "        self.outc = nn.Conv3d(64, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        x = self.up1(x3)\n",
        "        # Concatenate skip connection from encoder\n",
        "        x = torch.cat([x, x2], dim=1)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.up2(x)\n",
        "        # Concatenate skip connection from encoder\n",
        "        x = torch.cat([x, x1], dim=1)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "def train_model(root):\n",
        "    train_ds = HVSMR3DDataset(root, \"train\")\n",
        "    # Using batch_size > 1 is recommended if GPU memory allows\n",
        "    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = UNet3D()\n",
        "    model.to(device)\n",
        "\n",
        "    # It's good practice to use a slightly more robust optimizer and scheduler\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=1e-4) # Lower learning rate\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Increased number of epochs for better convergence\n",
        "    for epoch in range(25): # Increased from 10 to 25\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for img, msk in train_loader:\n",
        "            img, msk = img.to(device), msk.to(device)\n",
        "\n",
        "            pred = model(img)\n",
        "            loss = loss_fn(pred, msk)\n",
        "\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/25 completed. Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"rvsc_3d_unet.pth\")\n",
        "    print(\"✅ Model training complete and saved.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "F-4Z_fJKZKT-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Old Code\n",
        "#Do not run it\n",
        "class HVSMR3DDataset(Dataset):\n",
        "    def __init__(self, root, split=\"train\"):\n",
        "        images = sorted(glob.glob(os.path.join(root, split, \"images\", \"*.nii\")))\n",
        "        masks  = sorted(glob.glob(os.path.join(root, split, \"masks\", \"*.nii\")))\n",
        "        self.pairs = list(zip(images, masks))\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = nib.load(self.pairs[idx][0]).get_fdata().astype(np.float32)\n",
        "        msk = nib.load(self.pairs[idx][1]).get_fdata().astype(np.int64)\n",
        "        # Resize to a fixed size, e.g., 128x128x128\n",
        "        target_shape = (128, 128, 128)\n",
        "        img = zoom(img, np.array(target_shape) / img.shape, order=1) # Use order=1 for image interpolation\n",
        "        msk = zoom(msk, np.array(target_shape) / msk.shape, order=0) # Use order=0 for mask (nearest neighbor)\n",
        "        img = (img - img.mean()) / img.std()\n",
        "        return torch.unsqueeze(torch.from_numpy(img),0), torch.from_numpy(msk)\n",
        "\n",
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, in_ch=1, out_ch=2):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Conv3d(in_ch,16,3,padding=1), nn.ReLU(),\n",
        "            nn.Conv3d(16,32,3,padding=1), nn.ReLU())\n",
        "        self.pool = nn.MaxPool3d(2)\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.ConvTranspose3d(32,16,2,stride=2), nn.ReLU(),\n",
        "            nn.Conv3d(16,out_ch,1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.enc(x)\n",
        "        x2 = self.pool(x1)\n",
        "        x3 = self.dec(x2)\n",
        "        return x3\n",
        "\n",
        "def train_model(root):\n",
        "    train_ds = HVSMR3DDataset(root, \"train\")\n",
        "    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
        "    model = UNet3D()\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        for img, msk in train_loader:\n",
        "            img, msk = img.to(device), msk.to(device)\n",
        "            pred = model(img)\n",
        "            loss = loss_fn(pred, msk)\n",
        "            optim.zero_grad(); loss.backward(); optim.step()\n",
        "        print(f\"Epoch {epoch+1} completed.\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"rvsc_3d_unet.pth\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "J4wa8YsBQUax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_3d_mesh_from_nii(nii_path, label=1):\n",
        "    img = nib.load(nii_path).get_fdata()\n",
        "    verts, faces, normals, _ = measure.marching_cubes(img == label, level=0.5)\n",
        "    mesh = Mesh([verts, faces]).c(\"cyan\").alpha(0.5)\n",
        "    plt = Plotter(title=\"3D Mesh\", axes=1, bg='black')\n",
        "    plt.show(mesh)"
      ],
      "metadata": {
        "id": "EaY6ETgjQk-X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    zip_path = \"/content/RVSC.zip\"\n",
        "    extract_to = \"/content/RVSC_unzipped\"\n",
        "    unzip_rvsc_dataset(zip_path, extract_to)\n",
        "\n",
        "    # List contents of the extracted directory to diagnose the error\n",
        "    print(\"Contents of extracted directory:\")\n",
        "    print(os.listdir(extract_to))\n",
        "\n",
        "    convert_all_patients_to_nifti(\n",
        "        input_root=os.path.join(extract_to, \"RVSC\", \"TrainingSet\"),\n",
        "        output_root=\"/content/nifti_output\"\n",
        "    )\n",
        "\n",
        "    convert_nii_gz_to_nii(\"/content/nifti_output\")\n",
        "    organize_dataset_for_training(\"/content/nifti_output\", \"/content/rvsc_dataset\")\n",
        "\n",
        "    model = train_model(\"/content/rvsc_dataset\")\n",
        "\n",
        "    sample_mask = glob.glob(\"/content/rvsc_dataset/train/masks/*.nii\")[0] # Moved this line\n",
        "    generate_3d_mesh_from_nii(sample_mask, label=1)\n",
        "main()"
      ],
      "metadata": {
        "id": "lVGdmF50QxyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f59a550-030e-43fc-cb14-52ed9e398fb4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extracted dataset to: /content/RVSC_unzipped\n",
            "Contents of extracted directory:\n",
            "['RVSC']\n",
            "Converted and removed: /content/nifti_output/P10/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P10/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P04/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P04/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P07/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P07/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P14/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P14/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P16/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P16/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P05/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P05/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P08/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P08/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P13/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P13/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P06/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P06/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P11/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P11/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P12/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P12/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P03/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P03/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P15/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P15/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P02/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P02/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P01/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P01/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P09/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P09/image.nii.gz\n",
            "✅ Dataset organized for training.\n",
            "Using device: cuda\n",
            "Epoch 1/25 completed. Average Loss: 0.4624\n",
            "Epoch 2/25 completed. Average Loss: 0.2959\n",
            "Epoch 3/25 completed. Average Loss: 0.2492\n",
            "Epoch 4/25 completed. Average Loss: 0.2243\n",
            "Epoch 5/25 completed. Average Loss: 0.2097\n",
            "Epoch 6/25 completed. Average Loss: 0.1954\n",
            "Epoch 7/25 completed. Average Loss: 0.1828\n",
            "Epoch 8/25 completed. Average Loss: 0.1712\n",
            "Epoch 9/25 completed. Average Loss: 0.1619\n",
            "Epoch 10/25 completed. Average Loss: 0.1530\n",
            "Epoch 11/25 completed. Average Loss: 0.1433\n",
            "Epoch 12/25 completed. Average Loss: 0.1367\n",
            "Epoch 13/25 completed. Average Loss: 0.1299\n",
            "Epoch 14/25 completed. Average Loss: 0.1336\n",
            "Epoch 15/25 completed. Average Loss: 0.1289\n",
            "Epoch 16/25 completed. Average Loss: 0.1199\n",
            "Epoch 17/25 completed. Average Loss: 0.1115\n",
            "Epoch 18/25 completed. Average Loss: 0.1058\n",
            "Epoch 19/25 completed. Average Loss: 0.1013\n",
            "Epoch 20/25 completed. Average Loss: 0.0973\n",
            "Epoch 21/25 completed. Average Loss: 0.0940\n",
            "Epoch 22/25 completed. Average Loss: 0.0901\n",
            "Epoch 23/25 completed. Average Loss: 0.0866\n",
            "Epoch 24/25 completed. Average Loss: 0.0838\n",
            "Epoch 25/25 completed. Average Loss: 0.0809\n",
            "✅ Model training complete and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5534e0dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d56108-3282-4a92-92bf-34c4a0ae76b8"
      },
      "source": [
        "# Get the path to a sample image file from the organized dataset\n",
        "sample_image = glob.glob(\"/content/rvsc_dataset/train/images/*.nii\")[0]\n",
        "\n",
        "# Get the path to the ground truth mask file for the sample image\n",
        "# Assumes the sample_image variable is still holding the path to the image file\n",
        "# We need to find the corresponding mask file.\n",
        "# Based on the organize_dataset_for_training function, the mask file has the same prefix\n",
        "# as the image file but is in the 'masks' directory.\n",
        "sample_image_filename = os.path.basename(sample_image)\n",
        "sample_mask_filename = sample_image_filename.replace(\"_image.nii\", \"_mask.nii\")\n",
        "sample_mask_path = os.path.join(\"/content/rvsc_dataset/train/masks\", sample_mask_filename)\n",
        "\n",
        "def generate_3d_mesh_from_prediction(model_path, image_path, label=1, save_path=None):\n",
        "    model = UNet3D()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    img = nib.load(image_path).get_fdata().astype(np.float32)\n",
        "    # Preprocess the image similar to the dataset class\n",
        "    target_shape = (128, 128, 128)\n",
        "    img = zoom(img, np.array(target_shape) / img.shape, order=1)\n",
        "    img = (img - img.mean()) / img.std()\n",
        "    img = torch.unsqueeze(torch.unsqueeze(torch.from_numpy(img), 0), 0) # Add batch and channel dims\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(img)\n",
        "        pred = torch.argmax(output, dim=1).squeeze(0).numpy().astype(np.int16) # Cast to int16\n",
        "\n",
        "    print(f\"Prediction data range: min={pred.min()}, max={pred.max()}\") # Print the data range\n",
        "\n",
        "    # Adjust the level for marching cubes based on the range of prediction values\n",
        "    # The prediction is a mask with values 0 or 1.\n",
        "    # The level should be between 0 and 1 to create a surface at the boundary.\n",
        "    # A level of 0.5 is appropriate for binary masks.\n",
        "\n",
        "    # Check if the target label is present in the prediction\n",
        "    if label not in np.unique(pred):\n",
        "        print(f\"Warning: Target label {label} not found in the prediction. Cannot generate mesh.\")\n",
        "        return # Exit the function if the label is not found\n",
        "\n",
        "    verts, faces, normals, _ = measure.marching_cubes(pred == label, level=0.5)\n",
        "\n",
        "    mesh = Mesh([verts, faces]).c(\"cyan\").alpha(0.5)\n",
        "    if save_path:\n",
        "        mesh.write(save_path)\n",
        "        print(f\"Saved mesh to {save_path}\")\n",
        "    plt = Plotter(title=\"3D Mesh from Prediction\", axes=1, bg='black')\n",
        "    plt.show(mesh)\n",
        "\n",
        "\n",
        "# Generate the mesh from the ground truth mask\n",
        "generate_3d_mesh_from_nii(sample_mask_path, label=1)\n",
        "\n",
        "# Generate the mesh from the model prediction\n",
        "generate_3d_mesh_from_prediction(\n",
        "    \"/content/rvsc_3d_unet.pth\",\n",
        "    sample_image,\n",
        "    label=1,\n",
        "    save_path=\"/content/predicted_heart.ply\" )\n",
        "# or .stl"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction data range: min=0, max=0\n",
            "Warning: Target label 1 not found in the prediction. Cannot generate mesh.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import glob\n",
        "#import os\n",
        "\n",
        "# Get the path to a sample image file from the organized dataset\n",
        "#sample_image = glob.glob(\"/content/rvsc_dataset/train/images/*.nii\")[0]\n",
        "\n",
        "#generate_3d_mesh_from_prediction(\n",
        "#    \"/content/rvsc_3d_unet.pth\",\n",
        "#    sample_image,\n",
        "#    label=1,\n",
        "#    save_path=\"/content/predicted_heart.ply\" )\n",
        "# or .stl"
      ],
      "metadata": {
        "id": "we6D_JAoux1b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNFFBq3AxsgSzYcybmU7VHX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}