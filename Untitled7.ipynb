{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soutrik-Chakraborty/cardiac-modeling/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtEErX_Z5lO1",
        "outputId": "beb4072e-874e-4591-fc06-79a0adb85964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Collecting vedo\n",
            "  Downloading vedo-2025.5.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.16.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Collecting vtk (from vedo)\n",
            "  Downloading vtk-9.5.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: Pygments in /usr/local/lib/python3.11/dist-packages (from vedo) (2.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from vtk->vedo) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk->vedo) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk->vedo) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk->vedo) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk->vedo) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk->vedo) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk->vedo) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->vtk->vedo) (1.17.0)\n",
            "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vedo-2025.5.4-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vtk-9.5.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (112.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, vtk, nvidia-cusolver-cu12, vedo\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 pydicom-3.0.1 vedo-2025.5.4 vtk-9.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install pydicom nibabel opencv-python numpy torch torchvision scikit-image vedo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pydicom\n",
        "import nibabel as nib\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.ndimage import zoom\n",
        "from skimage import measure\n",
        "from vedo import Plotter, Mesh, Volume"
      ],
      "metadata": {
        "id": "N5VAG4I9P1Kv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip_rvsc_dataset(zip_path, extract_to):\n",
        "    if not os.path.isfile(zip_path):\n",
        "        raise FileNotFoundError(f\"Zip file not found at: {zip_path}\")\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(f\"\\u2705 Extracted dataset to: {extract_to}\")"
      ],
      "metadata": {
        "id": "SQg1ChGARJIQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dicom_series(dicom_folder):\n",
        "    slices = []\n",
        "    for fname in sorted(os.listdir(dicom_folder)):\n",
        "        if fname.endswith(\".dcm\"):\n",
        "            path = os.path.join(dicom_folder, fname)\n",
        "            dcm = pydicom.dcmread(path)\n",
        "            slices.append((dcm, int(fname.split('-')[1].split('.')[0])))\n",
        "    slices.sort(key=lambda x: x[1])\n",
        "    images = np.stack([s.pixel_array for s, _ in slices])\n",
        "    spacing = float(slices[0][0].PixelSpacing[0])\n",
        "    # Calculate slice thickness. Handle potential missing or inconsistent SliceLocation\n",
        "    thickness = 0.0\n",
        "    if len(slices) > 1 and hasattr(slices[0][0], 'SliceLocation') and hasattr(slices[1][0], 'SliceLocation'):\n",
        "        thickness = abs(slices[1][0].SliceLocation - slices[0][0].SliceLocation)\n",
        "    if thickness == 0.0 and hasattr(slices[0][0], 'SliceThickness'):\n",
        "         thickness = float(slices[0][0].SliceThickness)\n",
        "    if thickness == 0.0:\n",
        "        print(\"Warning: Could not determine slice thickness from DICOM headers. Using a default value of 1.0.\")\n",
        "        thickness = 1.0 # Default value if thickness cannot be determined\n",
        "\n",
        "\n",
        "    return images, spacing, thickness, [s[1] for s in slices]\n",
        "\n",
        "def load_contour_file(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        coords = [list(map(float, line.strip().split())) for line in f if line.strip()]\n",
        "    if not coords:\n",
        "        return np.array([], dtype=np.int32)\n",
        "    return np.array(coords, dtype=np.int32)\n",
        "\n",
        "def create_mask(image_shape, contour):\n",
        "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
        "    if contour.size > 0:\n",
        "        contour = contour.reshape((-1, 1, 2)).astype(np.int32)\n",
        "        cv2.fillPoly(mask, [contour], color=1)\n",
        "    return mask\n",
        "\n",
        "def contours_to_mask(contour_folder, dicom_numbers, image_shape, label='i'):\n",
        "    mask_volume = np.zeros((len(dicom_numbers), *image_shape), dtype=np.uint8)\n",
        "    for idx, num in enumerate(dicom_numbers):\n",
        "        fname = f\"{label}contour-manual.txt\"\n",
        "        files = glob.glob(os.path.join(contour_folder, f\"*{num:04d}*{fname}\"))\n",
        "        if files:\n",
        "            contour = load_contour_file(files[0])\n",
        "            mask = create_mask(image_shape, contour)\n",
        "            mask_volume[idx] = mask\n",
        "    return mask_volume\n",
        "\n",
        "def save_as_nifti(volume, output_path, spacing, slice_thickness):\n",
        "    affine = np.diag([spacing, spacing, slice_thickness, 1])\n",
        "    # Ensure determinant is not zero for affine decomposition\n",
        "    if np.linalg.det(affine) == 0:\n",
        "        print(f\"Warning: Affine determinant is zero. Using identity matrix with spacing for NIfTI saving.\")\n",
        "        affine = np.diag([spacing, spacing, slice_thickness, 1]) # Recreate affine just in case, though logic above should prevent det=0\n",
        "        # Fallback to a robust affine if the above still fails - this might not be ideal for all data but prevents crashing\n",
        "        if np.linalg.det(affine) == 0:\n",
        "             affine = np.eye(4)\n",
        "             affine[0,0] = spacing\n",
        "             affine[1,1] = spacing\n",
        "             affine[2,2] = slice_thickness\n",
        "\n",
        "    nib.save(nib.Nifti1Image(volume.astype(np.int16), affine), output_path)\n",
        "\n",
        "\n",
        "def convert_all_patients_to_nifti(input_root, output_root):\n",
        "    os.makedirs(output_root, exist_ok=True)\n",
        "    patient_dirs = sorted([d for d in os.listdir(input_root) if d.startswith(\"patient\")])\n",
        "    for patient in patient_dirs:\n",
        "        p_path = os.path.join(input_root, patient)\n",
        "        p_num = patient.replace(\"patient\", \"\")\n",
        "        dicom_folder = os.path.join(p_path, f\"P{p_num}dicom\")\n",
        "        contour_folder = os.path.join(p_path, f\"P{p_num}contours-manual\")\n",
        "        output_dir = os.path.join(output_root, f\"P{p_num}\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        try:\n",
        "            img, spc, thk, nums = load_dicom_series(dicom_folder)\n",
        "            msk = contours_to_mask(contour_folder, nums, img.shape[1:], label='i')\n",
        "            save_as_nifti(img, os.path.join(output_dir, \"image.nii.gz\"), spc, thk)\n",
        "            save_as_nifti(msk, os.path.join(output_dir, \"mask.nii.gz\"), spc, thk)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {patient}: {e}\")"
      ],
      "metadata": {
        "id": "xs2Byx-JP3f8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_nii_gz_to_nii(folder):\n",
        "    nii_gz_files = glob.glob(os.path.join(folder, \"**\", \"*.nii.gz\"), recursive=True)\n",
        "    for gz in nii_gz_files:\n",
        "        data = nib.load(gz)\n",
        "        nii_path = gz.replace(\".nii.gz\", \".nii\")\n",
        "        nib.save(data, nii_path)\n",
        "        os.remove(gz)\n",
        "        print(f\"Converted and removed: {gz}\")"
      ],
      "metadata": {
        "id": "f0HJJWnYQHQU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def organize_dataset_for_training(nifti_root, output_root):\n",
        "    os.makedirs(os.path.join(output_root, \"train\", \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_root, \"train\", \"masks\"), exist_ok=True)\n",
        "    patient_folders = sorted(glob.glob(os.path.join(nifti_root, \"P*\")))\n",
        "    for i, pf in enumerate(patient_folders):\n",
        "        img = glob.glob(os.path.join(pf, \"image.nii\"))[0]\n",
        "        msk = glob.glob(os.path.join(pf, \"mask.nii\"))[0]\n",
        "        pid = f\"P{str(i+1).zfill(2)}\"\n",
        "        shutil.copy(img, os.path.join(output_root, \"train\", \"images\", f\"{pid}_image.nii\"))\n",
        "        shutil.copy(msk, os.path.join(output_root, \"train\", \"masks\", f\"{pid}_mask.nii\"))\n",
        "    print(\"✅ Dataset organized for training.\")"
      ],
      "metadata": {
        "id": "qlERMfj6QO2f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HVSMR3DDataset(Dataset):\n",
        "    def __init__(self, root, split=\"train\"):\n",
        "        images = sorted(glob.glob(os.path.join(root, split, \"images\", \"*.nii\")))\n",
        "        masks  = sorted(glob.glob(os.path.join(root, split, \"masks\", \"*.nii\")))\n",
        "        self.pairs = list(zip(images, masks))\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = nib.load(self.pairs[idx][0]).get_fdata().astype(np.float32)\n",
        "        msk = nib.load(self.pairs[idx][1]).get_fdata().astype(np.int64)\n",
        "        # Resize to a fixed size, e.g., 128x128x128\n",
        "        target_shape = (128, 128, 128)\n",
        "        img = zoom(img, np.array(target_shape) / img.shape, order=1) # Use order=1 for image interpolation\n",
        "        msk = zoom(msk, np.array(target_shape) / msk.shape, order=0) # Use order=0 for mask (nearest neighbor)\n",
        "        img = (img - img.mean()) / img.std()\n",
        "        return torch.unsqueeze(torch.from_numpy(img),0), torch.from_numpy(msk)\n",
        "\n",
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, in_ch=1, out_ch=2):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Conv3d(in_ch,16,3,padding=1), nn.ReLU(),\n",
        "            nn.Conv3d(16,32,3,padding=1), nn.ReLU())\n",
        "        self.pool = nn.MaxPool3d(2)\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.ConvTranspose3d(32,16,2,stride=2), nn.ReLU(),\n",
        "            nn.Conv3d(16,out_ch,1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.enc(x)\n",
        "        x2 = self.pool(x1)\n",
        "        x3 = self.dec(x2)\n",
        "        return x3\n",
        "\n",
        "def train_model(root):\n",
        "    train_ds = HVSMR3DDataset(root, \"train\")\n",
        "    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
        "    model = UNet3D()\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        for img, msk in train_loader:\n",
        "            img, msk = img.to(device), msk.to(device)\n",
        "            pred = model(img)\n",
        "            loss = loss_fn(pred, msk)\n",
        "            optim.zero_grad(); loss.backward(); optim.step()\n",
        "        print(f\"Epoch {epoch+1} completed.\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"rvsc_3d_unet.pth\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "J4wa8YsBQUax"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_3d_mesh_from_nii(nii_path, label=1):\n",
        "    img = nib.load(nii_path).get_fdata()\n",
        "    verts, faces, normals, _ = measure.marching_cubes(img == label, level=0.5)\n",
        "    mesh = Mesh([verts, faces]).c(\"cyan\").alpha(0.5)\n",
        "    plt = Plotter(title=\"3D Mesh\", axes=1, bg='black')\n",
        "    plt.show(mesh)"
      ],
      "metadata": {
        "id": "EaY6ETgjQk-X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    zip_path = \"/content/RVSC.zip\"\n",
        "    extract_to = \"/content/RVSC_unzipped\"\n",
        "    unzip_rvsc_dataset(zip_path, extract_to)\n",
        "\n",
        "    # List contents of the extracted directory to diagnose the error\n",
        "    print(\"Contents of extracted directory:\")\n",
        "    print(os.listdir(extract_to))\n",
        "\n",
        "    convert_all_patients_to_nifti(\n",
        "        input_root=os.path.join(extract_to, \"RVSC\", \"TrainingSet\"),\n",
        "        output_root=\"/content/nifti_output\"\n",
        "    )\n",
        "\n",
        "    convert_nii_gz_to_nii(\"/content/nifti_output\")\n",
        "    organize_dataset_for_training(\"/content/nifti_output\", \"/content/rvsc_dataset\")\n",
        "\n",
        "    model = train_model(\"/content/rvsc_dataset\")\n",
        "\n",
        "    sample_mask = glob.glob(\"/content/rvsc_dataset/train/masks/*.nii\")[0] # Moved this line\n",
        "    generate_3d_mesh_from_nii(sample_mask, label=1)\n",
        "main()"
      ],
      "metadata": {
        "id": "lVGdmF50QxyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6190899f-c3e9-4616-db7e-e66b61e81607"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extracted dataset to: /content/RVSC_unzipped\n",
            "Contents of extracted directory:\n",
            "['RVSC']\n",
            "Converted and removed: /content/nifti_output/P08/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P08/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P02/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P02/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P16/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P16/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P09/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P09/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P06/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P06/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P10/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P10/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P11/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P11/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P05/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P05/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P13/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P13/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P14/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P14/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P01/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P01/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P07/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P07/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P04/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P04/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P15/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P15/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P12/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P12/mask.nii.gz\n",
            "Converted and removed: /content/nifti_output/P03/image.nii.gz\n",
            "Converted and removed: /content/nifti_output/P03/mask.nii.gz\n",
            "✅ Dataset organized for training.\n",
            "Epoch 1 completed.\n",
            "Epoch 2 completed.\n",
            "Epoch 3 completed.\n",
            "Epoch 4 completed.\n",
            "Epoch 5 completed.\n",
            "Epoch 6 completed.\n",
            "Epoch 7 completed.\n",
            "Epoch 8 completed.\n",
            "Epoch 9 completed.\n",
            "Epoch 10 completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5534e0dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d70451-1c87-4d5c-97bb-cc44c1cfe4d5"
      },
      "source": [
        "# Get the path to a sample image file from the organized dataset\n",
        "sample_image = glob.glob(\"/content/rvsc_dataset/train/images/*.nii\")[0]\n",
        "\n",
        "# Get the path to the ground truth mask file for the sample image\n",
        "# Assumes the sample_image variable is still holding the path to the image file\n",
        "# We need to find the corresponding mask file.\n",
        "# Based on the organize_dataset_for_training function, the mask file has the same prefix\n",
        "# as the image file but is in the 'masks' directory.\n",
        "sample_image_filename = os.path.basename(sample_image)\n",
        "sample_mask_filename = sample_image_filename.replace(\"_image.nii\", \"_mask.nii\")\n",
        "sample_mask_path = os.path.join(\"/content/rvsc_dataset/train/masks\", sample_mask_filename)\n",
        "\n",
        "def generate_3d_mesh_from_prediction(model_path, image_path, label=1, save_path=None):\n",
        "    model = UNet3D()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    img = nib.load(image_path).get_fdata().astype(np.float32)\n",
        "    # Preprocess the image similar to the dataset class\n",
        "    target_shape = (128, 128, 128)\n",
        "    img = zoom(img, np.array(target_shape) / img.shape, order=1)\n",
        "    img = (img - img.mean()) / img.std()\n",
        "    img = torch.unsqueeze(torch.unsqueeze(torch.from_numpy(img), 0), 0) # Add batch and channel dims\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(img)\n",
        "        pred = torch.argmax(output, dim=1).squeeze(0).numpy().astype(np.int16) # Cast to int16\n",
        "\n",
        "    print(f\"Prediction data range: min={pred.min()}, max={pred.max()}\") # Print the data range\n",
        "\n",
        "    # Adjust the level for marching cubes based on the range of prediction values\n",
        "    # The prediction is a mask with values 0 or 1.\n",
        "    # The level should be between 0 and 1 to create a surface at the boundary.\n",
        "    # A level of 0.5 is appropriate for binary masks.\n",
        "\n",
        "    # Check if the target label is present in the prediction\n",
        "    if label not in np.unique(pred):\n",
        "        print(f\"Warning: Target label {label} not found in the prediction. Cannot generate mesh.\")\n",
        "        return # Exit the function if the label is not found\n",
        "\n",
        "    verts, faces, normals, _ = measure.marching_cubes(pred == label, level=0.5)\n",
        "\n",
        "    mesh = Mesh([verts, faces]).c(\"cyan\").alpha(0.5)\n",
        "    if save_path:\n",
        "        mesh.write(save_path)\n",
        "        print(f\"Saved mesh to {save_path}\")\n",
        "    plt = Plotter(title=\"3D Mesh from Prediction\", axes=1, bg='black')\n",
        "    plt.show(mesh)\n",
        "\n",
        "\n",
        "# Generate the mesh from the ground truth mask\n",
        "generate_3d_mesh_from_nii(sample_mask_path, label=1)\n",
        "\n",
        "# Generate the mesh from the model prediction\n",
        "generate_3d_mesh_from_prediction(\n",
        "    \"/content/rvsc_3d_unet.pth\",\n",
        "    sample_image,\n",
        "    label=1,\n",
        "    save_path=\"/content/predicted_heart.ply\" )\n",
        "# or .stl"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction data range: min=0, max=0\n",
            "Warning: Target label 1 not found in the prediction. Cannot generate mesh.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import glob\n",
        "#import os\n",
        "\n",
        "# Get the path to a sample image file from the organized dataset\n",
        "#sample_image = glob.glob(\"/content/rvsc_dataset/train/images/*.nii\")[0]\n",
        "\n",
        "#generate_3d_mesh_from_prediction(\n",
        "#    \"/content/rvsc_3d_unet.pth\",\n",
        "#    sample_image,\n",
        "#    label=1,\n",
        "#    save_path=\"/content/predicted_heart.ply\" )\n",
        "# or .stl"
      ],
      "metadata": {
        "id": "we6D_JAoux1b"
      },
      "execution_count": 1,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOJD31ExSBgFkDBCRrtsOfy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}